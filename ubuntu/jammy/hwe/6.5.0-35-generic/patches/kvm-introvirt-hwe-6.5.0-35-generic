Description: KVM IntroVirt patch for Ubuntu HWE kernel 6.5.0-35
 The KVM IntroVirt patch adds introspection support to KVM. This allows
 for inspection of virtual machine memory and manipulation of running processes in-guest.
 This patch is required for compatibility with the user-land IntroVirt library.
Author: AIS Inc., introvirt@ainfosec.com
Origin: upstream, https://github.com/IntroVirt/kvm-introvirt
Bug: https://github.com/IntroVirt/kvm-introvirt/issues
Last-Update: 2024-05-10
---
This patch header follows DEP-3: http://dep.debian.net/deps/dep3/
Index: kvm-introvirt/kernel/arch/x86/include/asm/kvm_host.h
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/include/asm/kvm_host.h
+++ kvm-introvirt/kernel/arch/x86/include/asm/kvm_host.h
@@ -445,6 +445,7 @@ struct kvm_mmu {
 			    struct x86_exception *exception);
 	int (*sync_spte)(struct kvm_vcpu *vcpu,
 			 struct kvm_mmu_page *sp, int i);
+	int (*set_pte_perms)(struct kvm_vcpu *vcpu, u64 gpa, uint8_t perms, uint8_t* original_perms);
 	struct kvm_mmu_root_info root;
 	union kvm_cpu_role cpu_role;
 	union kvm_mmu_page_role root_role;
@@ -1556,6 +1557,7 @@ struct kvm_x86_ops {
 	void (*vcpu_put)(struct kvm_vcpu *vcpu);
 
 	void (*update_exception_bitmap)(struct kvm_vcpu *vcpu);
+	void (*update_syscall_intercept)(struct kvm_vcpu *vcpu);
 	int (*get_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	int (*set_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);
 	u64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);
@@ -1732,6 +1734,12 @@ struct kvm_x86_ops {
 	 * Returns vCPU specific APICv inhibit reasons
 	 */
 	unsigned long (*vcpu_get_apicv_inhibit_reasons)(struct kvm_vcpu *vcpu);
+
+	/* IntroVirt patch */
+	int (*set_cr_monitor)(struct kvm_vcpu *vcpu, int cr, int mode);
+	int (*set_monitor_trap_flag)(struct kvm_vcpu *vcpu, bool enabled);
+	int (*set_invlpg_monitor)(struct kvm_vcpu *vcpu, bool enabled);
+	bool (*hap_permissions_allowed)(uint16_t perms);
 };
 
 struct kvm_x86_nested_ops {
Index: kvm-introvirt/kernel/arch/x86/kvm/emulate.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/emulate.c
+++ kvm-introvirt/kernel/arch/x86/kvm/emulate.c
@@ -2456,6 +2456,69 @@ static int em_syscall(struct x86_emulate
 	return X86EMUL_CONTINUE;
 }
 
+static int em_sysret(struct x86_emulate_ctxt *ctxt)
+{
+	const struct x86_emulate_ops *ops = ctxt->ops;
+	struct desc_struct cs, ss;
+	u64 msr_data, rcx;
+	u16 cs_sel, ss_sel;
+	u64 efer = 0;
+
+	/* syscall is not available in real mode */
+	if(ctxt->mode == X86EMUL_MODE_REAL || ctxt->mode == X86EMUL_MODE_VM86)
+		return emulate_ud(ctxt);
+
+	if(!(em_syscall_is_enabled(ctxt)))
+		return emulate_ud(ctxt);
+
+	if(ctxt->ops->cpl(ctxt) != 0) {
+		return emulate_gp(ctxt, 0);
+	}
+
+	//check if RCX is in canonical form
+	rcx = reg_read(ctxt, VCPU_REGS_RCX);
+	if(((rcx & 0xFFFF800000000000) != 0xFFFF800000000000)
+			&& ((rcx & 0x00007FFFFFFFFFFF) != rcx)) {
+		return emulate_gp(ctxt, 0);
+	}
+
+	ops->get_msr(ctxt, MSR_EFER, &efer);
+	setup_syscalls_segments(&cs, &ss);
+
+	if (!(efer & EFER_SCE))
+		return emulate_ud(ctxt);
+
+	ops->get_msr(ctxt, MSR_STAR, &msr_data);
+	msr_data >>= 48;
+
+	//setup code segment, atleast what is left to do.
+	//setup_syscalls_segments does most of the work for us
+	if(ctxt->mode == X86EMUL_MODE_PROT64) { //if longmode
+		cs_sel = (u16)((msr_data + 0x10) | 0x3);
+		cs.l = 1;
+		cs.d = 0;
+	} else {
+		cs_sel = (u16)(msr_data | 0x3);
+		cs.l = 0;
+		cs.d = 1;
+	}
+	cs.dpl = 0x3;
+
+	//setup stack segment, atleast what is left to do.
+	//setup_syscalls_segments does most of the work for us
+	ss_sel = (u16)((msr_data + 0x8) | 0x3);
+	ss.dpl = 0x3;
+
+	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
+	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
+
+	ctxt->eflags = (reg_read(ctxt, VCPU_REGS_R11) & 0x3c7fd7) | 0x2;
+
+	ctxt->_eip = reg_read(ctxt, VCPU_REGS_RCX);
+
+	return X86EMUL_CONTINUE;
+}
+
 static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
@@ -4401,7 +4464,7 @@ static const struct opcode twobyte_table
 	/* 0x00 - 0x0F */
 	G(0, group6), GD(0, &group7), N, N,
 	N, I(ImplicitOps | EmulateOnUD | IsBranch, em_syscall),
-	II(ImplicitOps | Priv, em_clts, clts), N,
+	II(ImplicitOps | Priv, em_clts, clts), I(ImplicitOps | EmulateOnUD, em_sysret),
 	DI(ImplicitOps | Priv, invd), DI(ImplicitOps | Priv, wbinvd), N, N,
 	N, D(ImplicitOps | ModRM | SrcMem | NoAccess), N, N,
 	/* 0x10 - 0x1F */
Index: kvm-introvirt/kernel/arch/x86/kvm/irq.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/irq.c
+++ kvm-introvirt/kernel/arch/x86/kvm/irq.c
@@ -58,6 +58,9 @@ int kvm_cpu_has_extint(struct kvm_vcpu *
 	 * pending interrupt or should re-inject an injected
 	 * interrupt.
 	 */
+	if (v->monitor_trap_flag)
+		return 0;
+
 	if (!lapic_in_kernel(v))
 		return v->arch.interrupt.injected;
 
Index: kvm-introvirt/kernel/arch/x86/kvm/mmu/mmu.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/mmu/mmu.c
+++ kvm-introvirt/kernel/arch/x86/kvm/mmu/mmu.c
@@ -3749,6 +3749,71 @@ out_unlock:
 	return r;
 }
 
+static int tdp_set_pte_perms(struct kvm_vcpu *vcpu, u64 gfn, uint8_t perms, uint8_t* original_perms)
+{
+	struct kvm_shadow_walk_iterator iterator;
+	int result = -ESRCH;
+	u64 gpa = gfn << PAGE_SHIFT;
+	u64 spte = 0ull;
+
+	// Turn off any extra bits in the perms
+	perms &= (PT_USER_MASK|PT_WRITABLE_MASK|PT_PRESENT_MASK);
+
+	// Validate the HAP permissions are allowed
+	if (unlikely(!kvm_x86_ops.hap_permissions_allowed(perms)))
+		return -EINVAL;
+
+	if (!VALID_PAGE(vcpu->arch.mmu->root.hpa))
+	{
+		result = kvm_mmu_load(vcpu);
+		if (unlikely(result))
+			return result;
+	}
+
+retry:
+	walk_shadow_page_lockless_begin(vcpu);
+
+	/* Find the page in the shadow tables */
+	for_each_shadow_entry_lockless(vcpu, gpa, iterator, spte)
+		if (!is_shadow_present_pte(spte))
+		{
+			/* The page is not present, so page it in for the guest process */
+			struct mm_struct* mm;
+			struct kvm_page_fault fault = { .addr = gpa };
+			int r;
+
+			walk_shadow_page_lockless_end(vcpu);
+
+			/* Dirty hack */
+			/* hva_to_pfn() is hard-coded to current->mm */
+			mm = current->mm;
+			current->mm = vcpu->kvm->mm;
+			r = kvm_tdp_page_fault(vcpu, &fault);
+			current->mm = mm;
+			if (r)
+				return r;
+			goto retry;
+		}
+
+	// Try a few times to update the pte
+	result = -EBUSY;
+	do {
+		u64 new_spte = (spte & ~(PT_USER_MASK|PT_WRITABLE_MASK|PT_PRESENT_MASK)) | perms;
+
+		if (original_perms != NULL)
+			*original_perms = (spte & (PT_USER_MASK|PT_WRITABLE_MASK|PT_PRESENT_MASK));
+
+		if (cmpxchg64(iterator.sptep, spte, new_spte) == spte)
+		{
+			result = 0;
+			break;
+		}
+	} while(true);
+
+	walk_shadow_page_lockless_end(vcpu);
+	return result;
+}
+
 static int mmu_first_shadow_root_alloc(struct kvm *kvm)
 {
 	struct kvm_memslots *slots;
@@ -4519,7 +4584,7 @@ out_unlock:
 }
 #endif
 
-int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
+static int __kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
 {
 	/*
 	 * If the guest's MTRRs may be used to compute the "real" memtype,
@@ -4544,6 +4609,36 @@ int kvm_tdp_page_fault(struct kvm_vcpu *
 		}
 	}
 
+	/* If IntroVirt is hooking EPT faults, deliver here */
+	if (fault->guest_initiated && kvm_is_mem_event_enabled(vcpu->kvm))
+	{
+		/* Look up the fault */
+		struct kvm *kvm = vcpu->kvm;
+		struct mem_access_entry * entry;
+		const u64 gfn = fault->addr >> PAGE_SHIFT;
+		int found_match = 0;
+
+		mutex_lock(&kvm->mem_access_table.mutex);
+
+		/* Search for the entry */
+		hash_for_each_possible(kvm->mem_access_table.table, entry, node, gfn)
+		{
+			/* Found a match, check requested permission */
+			if(entry->gfn == gfn) {
+				/* Match! */
+				found_match = true;
+				break;
+			}
+		}
+		mutex_unlock(&kvm->mem_access_table.mutex);
+
+		/* If we found a match, deliver it */
+		if (found_match) {
+			kvm_deliver_mem_event(vcpu, fault->addr, fault->error_code);
+			return RET_PF_RETRY;
+		}
+	}
+
 #ifdef CONFIG_X86_64
 	if (tdp_mmu_enabled)
 		return kvm_tdp_mmu_page_fault(vcpu, fault);
@@ -4552,6 +4647,12 @@ int kvm_tdp_page_fault(struct kvm_vcpu *
 	return direct_page_fault(vcpu, fault);
 }
 
+int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)
+{
+	fault->guest_initiated = true;
+	return __kvm_tdp_page_fault(vcpu, fault);
+}
+
 static void nonpaging_init_context(struct kvm_mmu *context)
 {
 	context->page_fault = nonpaging_page_fault;
@@ -5249,6 +5350,7 @@ static void init_kvm_tdp_mmu(struct kvm_
 	context->cpu_role.as_u64 = cpu_role.as_u64;
 	context->root_role.word = root_role.word;
 	context->page_fault = kvm_tdp_page_fault;
+	context->set_pte_perms = tdp_set_pte_perms;
 	context->sync_spte = NULL;
 	context->get_guest_pgd = get_guest_cr3;
 	context->get_pdptr = kvm_pdptr_read;
@@ -5877,6 +5979,8 @@ void kvm_mmu_invlpg(struct kvm_vcpu *vcp
 	 */
 	kvm_mmu_invalidate_addr(vcpu, vcpu->arch.walk_mmu, gva, KVM_MMU_ROOTS_ALL);
 	++vcpu->stat.invlpg;
+
+	kvm_deliver_invlpg_event(vcpu, gva);
 }
 EXPORT_SYMBOL_GPL(kvm_mmu_invlpg);
 
Index: kvm-introvirt/kernel/arch/x86/kvm/mmu/mmu_internal.h
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/mmu/mmu_internal.h
+++ kvm-introvirt/kernel/arch/x86/kvm/mmu/mmu_internal.h
@@ -246,6 +246,9 @@ struct kvm_page_fault {
 	 * is changing its own translation in the guest page tables.
 	 */
 	bool write_fault_to_shadow_pgtable;
+
+	/* IntroVirt Patch */
+	bool guest_initiated;
 };
 
 int kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);
Index: kvm-introvirt/kernel/arch/x86/kvm/svm/svm.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/svm/svm.c
+++ kvm-introvirt/kernel/arch/x86/kvm/svm/svm.c
@@ -4772,6 +4772,26 @@ static bool svm_apic_init_signal_blocked
 	return !gif_set(svm);
 }
 
+static int svm_set_cr_monitor(struct kvm_vcpu* vcpu, int cr, int mode)
+{
+	return -ENODEV;
+}
+
+static int svm_set_monitor_trap_flag(struct kvm_vcpu* vcpu, bool enabled)
+{
+	return -ENODEV;
+}
+
+static int svm_set_invlpg_monitor(struct kvm_vcpu *vcpu, bool enabled)
+{
+	return -ENODEV;
+}
+
+static bool svm_hap_permissions_allowed(uint16_t perms)
+{
+	return false;
+}
+
 static void svm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)
 {
 	if (!sev_es_guest(vcpu->kvm))
@@ -4825,6 +4845,7 @@ static struct kvm_x86_ops svm_x86_ops __
 	.vcpu_unblocking = avic_vcpu_unblocking,
 
 	.update_exception_bitmap = svm_update_exception_bitmap,
+	.update_syscall_intercept = svm_update_exception_bitmap, // TODO: Change when we have introspection on SVM
 	.get_msr_feature = svm_get_msr_feature,
 	.get_msr = svm_get_msr,
 	.set_msr = svm_set_msr,
@@ -4931,6 +4952,12 @@ static struct kvm_x86_ops svm_x86_ops __
 
 	.vcpu_deliver_sipi_vector = svm_vcpu_deliver_sipi_vector,
 	.vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,
+
+	/* IntroVirt patch */
+	.set_cr_monitor = svm_set_cr_monitor,
+	.set_monitor_trap_flag = svm_set_monitor_trap_flag,
+	.set_invlpg_monitor = svm_set_invlpg_monitor,
+	.hap_permissions_allowed = svm_hap_permissions_allowed
 };
 
 /*
Index: kvm-introvirt/kernel/arch/x86/kvm/vmx/capabilities.h
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/vmx/capabilities.h
+++ kvm-introvirt/kernel/arch/x86/kvm/vmx/capabilities.h
@@ -71,6 +71,7 @@ extern struct vmcs_config vmcs_config __
 struct vmx_capability {
 	u32 ept;
 	u32 vpid;
+	bool mtf;
 };
 extern struct vmx_capability vmx_capability __ro_after_init;
 
@@ -116,6 +117,11 @@ static inline bool cpu_has_vmx_tpr_shado
 	return vmcs_config.cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW;
 }
 
+static inline bool cpu_has_monitor_trap_flag(void)
+{
+	return vmx_capability.mtf;
+}
+
 static inline bool cpu_need_tpr_shadow(struct kvm_vcpu *vcpu)
 {
 	return cpu_has_vmx_tpr_shadow() && lapic_in_kernel(vcpu);
Index: kvm-introvirt/kernel/arch/x86/kvm/vmx/vmx.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/vmx/vmx.c
+++ kvm-introvirt/kernel/arch/x86/kvm/vmx/vmx.c
@@ -865,7 +865,7 @@ void vmx_update_exception_bitmap(struct
 	 * We intercept those #GP and allow access to them anyway
 	 * as VMware does.
 	 */
-	if (enable_vmware_backdoor)
+	if (enable_vmware_backdoor | vcpu->syscall_hook_enabled)
 		eb |= (1u << GP_VECTOR);
 	if ((vcpu->guest_debug &
 	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==
@@ -1098,6 +1098,11 @@ static bool update_transition_efer(struc
 		ignore_bits &= ~(u64)EFER_SCE;
 #endif
 
+	/* IntroVirt patch to intercept system calls */
+	if (vmx->vcpu.syscall_hook_enabled) {
+		guest_efer &= ~EFER_SCE;
+	}
+
 	/*
 	 * On EPT, we can't emulate NX, so we must switch EFER atomically.
 	 * On CPUs that support "load IA32_EFER", always switch EFER
@@ -2012,7 +2017,8 @@ static int vmx_get_msr(struct kvm_vcpu *
 		msr_info->data = to_vmx(vcpu)->spec_ctrl;
 		break;
 	case MSR_IA32_SYSENTER_CS:
-		msr_info->data = vmcs_read32(GUEST_SYSENTER_CS);
+		/* IntroVirt Patch - Always return our shadow */
+		msr_info->data = vcpu->shadow_msr_ia32_sysenter_cs;
 		break;
 	case MSR_IA32_SYSENTER_EIP:
 		msr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);
@@ -2615,6 +2621,13 @@ static int setup_vmcs_config(struct vmcs
 				SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |
 				SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 
+	if (_cpu_based_exec_control & CPU_BASED_MONITOR_TRAP_FLAG) {
+		_cpu_based_exec_control &= ~CPU_BASED_MONITOR_TRAP_FLAG;
+		vmx_cap->mtf = true;
+	} else {
+		vmx_cap->mtf = false;
+	}
+
 	rdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,
 		&vmx_cap->ept, &vmx_cap->vpid);
 
@@ -5209,7 +5222,7 @@ static int handle_exception_nmi(struct k
 		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
 
 	if (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {
-		WARN_ON_ONCE(!enable_vmware_backdoor);
+		/*WARN_ON_ONCE(!enable_vmware_backdoor);*/
 
 		/*
 		 * VMware backdoor emulation on #GP interception only handles
@@ -5311,6 +5324,24 @@ static int handle_exception_nmi(struct k
 		kvm_run->exit_reason = KVM_EXIT_DEBUG;
 		kvm_run->debug.arch.pc = kvm_get_linear_rip(vcpu);
 		kvm_run->debug.arch.exception = ex_no;
+
+		/*
+		 * Check if we have an introspection client attached
+		 */
+		if(vcpu->introspection_mm)
+		{
+			if(ex_no == BP_VECTOR)
+			{
+				/*
+				 *  Deliver the event to the introspection handler,
+				 *  instead of exiting to QEMU.
+				 */
+				kvm_deliver_trap_event(vcpu, ex_no);
+
+				return 1;
+			}
+		}
+
 		break;
 	case AC_VECTOR:
 		if (vmx_guest_inject_ac(vcpu)) {
@@ -5453,19 +5484,25 @@ static int handle_cr(struct kvm_vcpu *vc
 		trace_kvm_cr_write(cr, val);
 		switch (cr) {
 		case 0:
+			kvm_deliver_cr_write_event(vcpu, 0, val);
 			err = handle_set_cr0(vcpu, val);
+			/* These events are delivered here so that we know they're coming from the guest
+			 * kvm_set_cr0() would work as well, but we'd get events when it's set via ioctl.
+			 */
 			return kvm_complete_insn_gp(vcpu, err);
 		case 3:
-			WARN_ON_ONCE(enable_unrestricted_guest);
-
+			/* WARN_ON_ONCE(enable_unrestricted_guest); */
+			kvm_deliver_cr_write_event(vcpu, 3, val);
 			err = kvm_set_cr3(vcpu, val);
 			return kvm_complete_insn_gp(vcpu, err);
 		case 4:
+			kvm_deliver_cr_write_event(vcpu, 4, val);
 			err = handle_set_cr4(vcpu, val);
 			return kvm_complete_insn_gp(vcpu, err);
 		case 8: {
 				u8 cr8_prev = kvm_get_cr8(vcpu);
 				u8 cr8 = (u8)val;
+				kvm_deliver_cr_write_event(vcpu, 8, val);
 				err = kvm_set_cr8(vcpu, cr8);
 				ret = kvm_complete_insn_gp(vcpu, err);
 				if (lapic_in_kernel(vcpu))
@@ -5493,11 +5530,13 @@ static int handle_cr(struct kvm_vcpu *vc
 			val = kvm_read_cr3(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
+			kvm_deliver_cr_read_event(vcpu, 3, val);
 			return kvm_skip_emulated_instruction(vcpu);
 		case 8:
 			val = kvm_get_cr8(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
+			kvm_deliver_cr_read_event(vcpu, 8, val);
 			return kvm_skip_emulated_instruction(vcpu);
 		}
 		break;
@@ -5579,6 +5618,21 @@ out:
 	return kvm_complete_insn_gp(vcpu, err);
 }
 
+static int handle_cpuid(struct kvm_vcpu *vcpu)
+{
+	/*
+	 * It was difficult to find an event that is triggered on a reboot only once.
+	 * After we see RIP jumped to 0xFFF0, we wait for a CPUID instruction to
+	 * detect a "reboot" event.
+	 */
+	if (unlikely(vcpu->reboot_pending)) {
+		if (vcpu->vcpu_id == 0)
+			kvm_deliver_reboot_event(vcpu);
+		vcpu->reboot_pending = false;
+	}
+	return kvm_emulate_cpuid(vcpu);
+}
+
 static void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)
 {
 	get_debugreg(vcpu->arch.db[0], 0);
@@ -5932,6 +5986,11 @@ static int handle_pause(struct kvm_vcpu
 
 static int handle_monitor_trap(struct kvm_vcpu *vcpu)
 {
+	/*
+	* If introspection is enabled, deliver the event.
+	*/
+	if(vcpu->introspection_mm)
+		kvm_deliver_monitor_trap_flag_event(vcpu);
 	return 1;
 }
 
@@ -6083,7 +6142,7 @@ static int (*kvm_vmx_exit_handlers[])(st
 	[EXIT_REASON_IO_INSTRUCTION]          = handle_io,
 	[EXIT_REASON_CR_ACCESS]               = handle_cr,
 	[EXIT_REASON_DR_ACCESS]               = handle_dr,
-	[EXIT_REASON_CPUID]                   = kvm_emulate_cpuid,
+	[EXIT_REASON_CPUID]                   = handle_cpuid,
 	[EXIT_REASON_MSR_READ]                = kvm_emulate_rdmsr,
 	[EXIT_REASON_MSR_WRITE]               = kvm_emulate_wrmsr,
 	[EXIT_REASON_INTERRUPT_WINDOW]        = handle_interrupt_window,
@@ -7501,7 +7560,8 @@ static int vmx_vcpu_create(struct kvm_vc
 	vmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);
 	vmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);
 #endif
-	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);
+	/* IntroVirt Patch */
+	/* vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW); */
 	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);
 	vmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);
 	if (kvm_cstate_in_guest(vcpu->kvm)) {
@@ -8180,6 +8240,102 @@ static void vmx_hardware_unsetup(void)
 	free_kvm_area();
 }
 
+int vmx_set_cr_monitor(struct kvm_vcpu *vcpu, int cr, int mode)
+{
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (unlikely(mode > 0x3)) {
+		printk ("Invalid mode 0x%X passed to KVM_SET_CR_MONITOR\n", mode);
+		return -EINVAL;
+	}
+
+	switch (cr)
+	{
+		// We can only monitor writes to CR 0 and 4
+		case 0:
+		case 4:
+			if (mode & KVM_MONITOR_CR_READ) {
+				printk(KERN_WARNING "Tried to enable CR%d read monitoring, but unsupported\n", cr);
+				return -ENODEV;
+			}
+			break;
+		case 3:
+			exec_controls_clearbit(vmx, CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);
+
+			if (mode & KVM_MONITOR_CR_WRITE)
+				exec_controls_setbit(vmx, CPU_BASED_CR3_LOAD_EXITING);
+			if (mode & KVM_MONITOR_CR_READ)
+				exec_controls_setbit(vmx, CPU_BASED_CR3_STORE_EXITING);
+			break;
+		case 8:
+			exec_controls_clearbit(vmx, CPU_BASED_CR8_LOAD_EXITING | CPU_BASED_CR8_STORE_EXITING);
+
+			if (mode & KVM_MONITOR_CR_WRITE)
+				exec_controls_setbit(vmx, CPU_BASED_CR8_LOAD_EXITING);
+			if (mode & KVM_MONITOR_CR_READ)
+				exec_controls_setbit(vmx, CPU_BASED_CR8_STORE_EXITING);
+			break;
+		default:
+			return -ENODEV;
+	}
+
+	// Update the VCPU mask, checked by event delivery
+	vcpu->cr_monitor_mask &= ~(0x3 << (cr * 2));
+	vcpu->cr_monitor_mask |= (mode << (cr * 2));
+
+	if (!enable_ept)
+	{
+		// Without EPT, we always need CR3 intercepts
+		exec_controls_setbit(vmx, CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);
+	}
+
+	return 0;
+}
+
+int vmx_set_monitor_trap_flag(struct kvm_vcpu *vcpu, bool enabled) {
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	if (!cpu_has_monitor_trap_flag())
+		return -ENODEV;
+
+	vcpu->monitor_trap_flag = enabled;
+
+	if (enabled)
+		exec_controls_setbit(vmx, CPU_BASED_MONITOR_TRAP_FLAG);
+	else
+		exec_controls_clearbit(vmx, CPU_BASED_MONITOR_TRAP_FLAG);
+	return 0;
+}
+
+int vmx_set_invlpg_monitor(struct kvm_vcpu *vcpu, bool enabled) {
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	/* Without ept, we always need it set */
+	if (enabled || !enable_ept)
+		exec_controls_setbit(vmx, CPU_BASED_INVLPG_EXITING);
+	else
+		exec_controls_clearbit(vmx, CPU_BASED_INVLPG_EXITING);
+
+	vcpu->invlpg_hook = enabled;
+
+	return 0;
+}
+
+bool vmx_hap_permissions_allowed(uint16_t perms) {
+	if ((perms & PT_PRESENT_MASK) == 0)
+	{
+		if ((perms & PT_WRITABLE_MASK))
+			// Not allowed to do write-only or write+execute.
+			// You have to have read for those.
+			return false;
+		else if ((perms & PT_USER_MASK))
+			// Execute only. This is only supported if a feature bit is set.
+			if (!cpu_has_vmx_ept_execute_only())
+				return false;
+	}
+	return true;
+}
+
 #define VMX_REQUIRED_APICV_INHIBITS			\
 (							\
 	BIT(APICV_INHIBIT_REASON_DISABLE)|		\
@@ -8223,6 +8379,7 @@ static struct kvm_x86_ops vmx_x86_ops __
 	.vcpu_put = vmx_vcpu_put,
 
 	.update_exception_bitmap = vmx_update_exception_bitmap,
+	.update_syscall_intercept = vmx_update_exception_bitmap,
 	.get_msr_feature = vmx_get_msr_feature,
 	.get_msr = vmx_get_msr,
 	.set_msr = vmx_set_msr,
@@ -8338,6 +8495,12 @@ static struct kvm_x86_ops vmx_x86_ops __
 	.complete_emulated_msr = kvm_complete_insn_gp,
 
 	.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,
+
+	/* IntroVirt patch */
+	.set_cr_monitor = vmx_set_cr_monitor,
+	.set_monitor_trap_flag = vmx_set_monitor_trap_flag,
+	.set_invlpg_monitor = vmx_set_invlpg_monitor,
+	.hap_permissions_allowed = vmx_hap_permissions_allowed,
 };
 
 static unsigned int vmx_handle_intel_pt_intr(void)
Index: kvm-introvirt/kernel/arch/x86/kvm/x86.c
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/x86.c
+++ kvm-introvirt/kernel/arch/x86/kvm/x86.c
@@ -127,6 +127,7 @@ static void __kvm_set_rflags(struct kvm_
 static void store_regs(struct kvm_vcpu *vcpu);
 static int sync_regs(struct kvm_vcpu *vcpu);
 static int kvm_vcpu_do_singlestep(struct kvm_vcpu *vcpu);
+void kvm_arch_clear_mem_access(struct kvm* kvm);
 
 static int __set_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
 static void __get_sregs2(struct kvm_vcpu *vcpu, struct kvm_sregs2 *sregs2);
@@ -146,6 +147,9 @@ EXPORT_STATIC_CALL_GPL(kvm_x86_cache_reg
 static bool __read_mostly ignore_msrs = 0;
 module_param(ignore_msrs, bool, S_IRUGO | S_IWUSR);
 
+static void init_emulate_ctxt(struct kvm_vcpu *vcpu);
+static void toggle_interruptibility(struct kvm_vcpu *vcpu, u32 mask);
+
 bool __read_mostly report_ignored_msrs = true;
 module_param(report_ignored_msrs, bool, S_IRUGO | S_IWUSR);
 EXPORT_SYMBOL_GPL(report_ignored_msrs);
@@ -1869,6 +1873,15 @@ static int __kvm_set_msr(struct kvm_vcpu
 		 */
 		data = __canonical_address(data, vcpu_virt_addr_bits(vcpu));
 		break;
+	case MSR_IA32_SYSENTER_CS:
+		/*
+		 * Any time there's a write to this MSR, update the shadow.
+		 * Also, switch it to 0 if we are intercepting system calls.
+		 */
+		vcpu->shadow_msr_ia32_sysenter_cs = data;
+		if (vcpu->syscall_hook_enabled)
+			data = 0;
+		break;
 	case MSR_TSC_AUX:
 		if (!kvm_is_supported_user_return_msr(MSR_TSC_AUX))
 			return 1;
@@ -4542,6 +4555,9 @@ int kvm_vm_ioctl_check_extension(struct
 			     KVM_XEN_HVM_CONFIG_RUNSTATE_UPDATE_FLAG;
 		break;
 #endif
+	case KVM_CAP_INTROSPECTION:
+		r = KVM_INTROSPECTION_API_VERSION;
+		break;
 	case KVM_CAP_SYNC_REGS:
 		r = KVM_SYNC_X86_VALID_FIELDS;
 		break;
@@ -5646,6 +5662,26 @@ static int kvm_vcpu_ioctl_enable_cap(str
 	}
 }
 
+static int kvm_set_syscall_hook(struct kvm_vcpu *vcpu, bool enabled)
+{
+	int rc = 0;
+	if (vcpu->syscall_hook_enabled == enabled)
+		goto done;
+
+	vcpu->syscall_hook_enabled = enabled;
+
+	// Update the efer using the new 'syscall_hook_enabled' setting.
+	kvm_x86_ops.set_efer(vcpu, vcpu->arch.efer);
+
+	// Update MSR_IA32_SYSENTER_CS
+	kvm_set_msr(vcpu, MSR_IA32_SYSENTER_CS, vcpu->shadow_msr_ia32_sysenter_cs);
+
+	// Turn on or off #GP intercept
+	kvm_x86_ops.update_syscall_intercept(vcpu);
+done:
+	return rc;
+}
+
 long kvm_arch_vcpu_ioctl(struct file *filp,
 			 unsigned int ioctl, unsigned long arg)
 {
@@ -6078,6 +6114,107 @@ long kvm_arch_vcpu_ioctl(struct file *fi
 	case KVM_SET_DEVICE_ATTR:
 		r = kvm_vcpu_ioctl_device_attr(vcpu, ioctl, argp);
 		break;
+	case KVM_SET_CR_MONITOR: {
+		struct kvm_cr_monitor cr_mon;
+
+		r = -EFAULT;
+		if (copy_from_user(&cr_mon, argp, sizeof(cr_mon)))
+			goto out;
+
+		r = kvm_x86_ops.set_cr_monitor(vcpu, cr_mon.cr, cr_mon.mode);
+		break;
+	}
+	case KVM_SET_INVLPG_HOOK: {
+		r = kvm_x86_ops.set_invlpg_monitor(vcpu, arg);
+		break;
+	}
+	case KVM_SET_MONITOR_TRAP_FLAG: {
+		r = kvm_x86_ops.set_monitor_trap_flag(vcpu, arg);
+		break;
+	}
+	case KVM_SET_SYSCALL_HOOK: {
+		r = kvm_set_syscall_hook(vcpu, arg);
+		break;
+	}
+	case KVM_SET_VMCALL_HOOK: {
+		r = 0;
+		vcpu->vmcall_hook_enabled = (arg != 0);
+		break;
+	}
+	case KVM_INJECT_TRAP: {
+		struct kvm_inject_trap trap;
+		r = -EFAULT;
+		if (copy_from_user(&trap, argp, sizeof(trap)))
+			goto out;
+
+		if(trap.vector == PF_VECTOR) {
+			vcpu->arch.cr2 = trap.cr2;
+		}
+		kvm_multiple_exception(vcpu, trap.vector, trap.has_error, trap.error_code, false, 0, false);
+		r = 0;
+
+		break;
+	}
+	case KVM_VCPU_INJECT_SYSENTER:
+	case KVM_VCPU_INJECT_SYSCALL: {
+		struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
+		uint8_t insn[8];
+		int insn_len = 0;
+		unsigned long rflags = kvm_x86_ops.get_rflags(vcpu);
+		struct kvm_segment cs;
+
+		// Check CS for long mode
+		kvm_get_segment(vcpu, &cs, VCPU_SREG_CS);
+
+		init_emulate_ctxt(vcpu);
+
+		ctxt->interruptibility = 0;
+		ctxt->have_exception = false;
+		ctxt->exception.vector = -1;
+		ctxt->perm_ok = false;
+
+		if (ioctl == KVM_VCPU_INJECT_SYSCALL)
+		{
+			// SYSCALL
+			if (cs.l)
+				insn[insn_len++] = 0x48;
+			insn[insn_len++] = 0x0F;
+			insn[insn_len++] = 0x05;
+		} else {
+			// SYSENTER
+			if (cs.l)
+				insn[insn_len++] = 0x48;
+			insn[insn_len++] = 0x0F;
+			insn[insn_len++] = 0x34;
+		}
+
+		r = x86_decode_insn(ctxt, insn, insn_len, 0);
+		if (r != EMULATION_OK) {
+			printk(KERN_WARNING "x86_decode_insn failed: %d\n", r);
+			break;
+		}
+
+		r = x86_emulate_insn(ctxt);
+		if (r != EMULATION_OK)
+			break;
+
+		kvm_rip_write(vcpu, ctxt->eip);
+		__kvm_set_rflags(vcpu, ctxt->eflags);
+		emulator_writeback_register_cache(ctxt);
+		vcpu->arch.emulate_regs_need_sync_to_vcpu = false;
+		toggle_interruptibility(vcpu, ctxt->interruptibility);
+
+		/*
+		 * For STI, interrupts are shadowed; so KVM_REQ_EVENT will
+		 * do nothing, and it will be requested again as soon as
+		 * the shadow expires.  But we still need to check here,
+		 * because POPF has no interrupt shadow.
+		 */
+		if (unlikely((ctxt->eflags & ~rflags) & X86_EFLAGS_IF))
+			kvm_make_request(KVM_REQ_EVENT, vcpu);
+
+		break;
+	}
 	default:
 		r = -EINVAL;
 	}
@@ -7105,6 +7242,92 @@ set_pit2_out:
 		r = kvm_vm_ioctl_set_msr_filter(kvm, &filter);
 		break;
 	}
+	case KVM_SET_MEM_ACCESS_ENABLED: {
+		kvm->mem_event_enabled = arg;
+
+		/* TODO: Check if EPT is enabled; it's the only one we support right now. */
+
+		r = 0;
+		if (!kvm->mem_event_enabled) {
+			/* Remove all entries */
+			kvm_arch_clear_mem_access(kvm);
+		}
+		break;
+	}
+	case KVM_SET_MEM_ACCESS: {
+		unsigned long i;
+		struct kvm_ept_permissions perms;
+		struct kvm_vcpu *vcpu = NULL;
+		struct mem_access_entry * entry;
+
+		r = -EINVAL;
+		if (unlikely(xa_empty(&kvm->vcpu_array)))
+			goto out;
+
+		r = -EINVAL;
+		if (unlikely(!kvm->mem_event_enabled))
+			goto out;
+
+		r = -EFAULT;
+		if (copy_from_user(&perms, argp, sizeof(perms)))
+			goto out;
+
+		mutex_lock(&kvm->mem_access_table.mutex);
+
+		kvm_for_each_vcpu(i, vcpu, kvm) {
+			break;  // Just need the first one
+		}
+
+		// Search for the entry
+		hash_for_each_possible(kvm->mem_access_table.table, entry, node, perms.gfn)
+		{
+			if (entry->gfn == perms.gfn)
+			{
+				/* This entry already exists */
+				/* Set the requested permissions */
+				r = vcpu->arch.mmu->set_pte_perms(vcpu, perms.gfn, perms.perms, NULL);
+				if (r)
+					goto set_ept_out;
+
+				if (perms.perms == entry->original_perms)
+				{
+					/* Setting back to original permissions, just remove the entry */
+					hash_del(&entry->node);
+					kfree(entry);
+					r = 0;
+				}
+				goto set_ept_out;
+			}
+		}
+
+		r = 0;
+		/* Don't bother adding an entry when setting full permissions */
+		if (perms.perms != (PT_USER_MASK|PT_WRITABLE_MASK|PT_PRESENT_MASK))
+		{
+			/* We didn't find an existing entry, add a new one. */
+			r = -ENOMEM;
+			entry = kmalloc(sizeof(struct mem_access_entry), GFP_KERNEL);
+			if(!entry)
+				goto set_ept_out;
+			entry->gfn = perms.gfn;
+
+			/* Update the PTE */
+			r = vcpu->arch.mmu->set_pte_perms(vcpu, perms.gfn, perms.perms, &entry->original_perms);
+			if (r)
+			{
+				// Failed to set hardware, so free the entry
+				kfree(entry);
+				goto set_ept_out;
+			}
+
+			/* Add the entry to the table */
+			hash_add(kvm->mem_access_table.table, &entry->node, perms.gfn);
+		}
+set_ept_out:
+		mutex_unlock(&kvm->mem_access_table.mutex);
+
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}
@@ -8893,6 +9116,8 @@ int x86_emulate_instruction(struct kvm_v
 	int r;
 	struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;
 	bool writeback = true;
+	bool sysret = false;
+	bool sysexit = false;
 
 	if (unlikely(!kvm_can_emulate_insn(vcpu, emulation_type, insn, insn_len)))
 		return 1;
@@ -8937,11 +9162,55 @@ int x86_emulate_instruction(struct kvm_v
 		}
 	}
 
-	if ((emulation_type & EMULTYPE_VMWARE_GP) &&
-	    !is_vmware_backdoor_opcode(ctxt)) {
-		kvm_queue_exception_e(vcpu, GP_VECTOR, 0);
-		return 1;
-	}
+	if (ctxt->d) {
+		int i = 0;
+
+		if(ctxt->fetch.data[0] == 0x48)
+			++i; // Skip over the REX.W prefix
+
+		if(ctxt->fetch.data[i] == 0x0F) {
+			switch(ctxt->fetch.data[i + 1]) {
+			case 0x05: { // SYSCALL
+				const u64 original_rip = kvm_rip_read(vcpu);
+				kvm_deliver_syscall_event(vcpu, original_rip + ctxt->opcode_len);
+				if (kvm_rip_read(vcpu) != original_rip || vcpu->arch.exception.pending)
+				    return 1;
+				break;
+			}
+			case 0x07: // SYSRET
+				sysret = true;
+				break;
+			}
+		}
+	} else if (emulation_type & EMULTYPE_VMWARE_GP) {
+		int i = 0;
+		bool handled = false;
+
+		if(ctxt->fetch.data[0] == 0x48)
+			++i; // Skip over the REX.W prefix
+
+		if(ctxt->fetch.data[i] == 0x0F) {
+			switch(ctxt->fetch.data[i + 1]) {
+			case 0x34: { /* SYSENTER */
+				const u64 original_rip = kvm_rip_read(vcpu);
+				kvm_deliver_sysenter_event(vcpu, original_rip + ctxt->opcode_len);
+				if (kvm_rip_read(vcpu) != original_rip || vcpu->arch.exception.pending)
+				    return 1;
+				handled = true;
+			    break;
+			}
+	        case 0x35: /* SYSEXIT */
+	            sysexit = true;
+				handled = true;
+	            break;
+			}
+		}
+
+		if (!handled && !is_vmware_backdoor_opcode(ctxt)) {
+			kvm_queue_exception_e(vcpu, GP_VECTOR, 0);
+			return 1;
+		}
+ 	}
 
 	/*
 	 * EMULTYPE_SKIP without EMULTYPE_COMPLETE_USER_EXIT is intended for
@@ -9066,6 +9335,11 @@ writeback:
 	} else
 		vcpu->arch.emulate_regs_need_sync_to_vcpu = true;
 
+	if (sysret)
+		kvm_deliver_sysret_event(vcpu);
+	else if (sysexit)
+		kvm_deliver_sysexit_event(vcpu);
+
 	return r;
 }
 
@@ -9636,6 +9910,46 @@ void kvm_x86_vendor_exit(void)
 }
 EXPORT_SYMBOL_GPL(kvm_x86_vendor_exit);
 
+void kvm_arch_clear_mem_access(struct kvm* kvm) {
+	unsigned long i;
+	struct kvm_vcpu * vcpu = NULL;
+	struct mem_access_entry * entry;
+	struct hlist_node * tmp;
+	int bkt;
+
+	/* Remove all entries */
+	mutex_lock(&kvm->mem_access_table.mutex);
+
+	kvm_for_each_vcpu(i, vcpu, kvm) {
+		break; // Just need first VCPU
+	}
+
+	hash_for_each_safe(kvm->mem_access_table.table, bkt, tmp, entry, node)
+	{
+		/* Set the mem_access values back to normal */
+		vcpu->arch.mmu->set_pte_perms(vcpu, entry->gfn, entry->original_perms, NULL);
+
+		hash_del(&entry->node);
+		kfree(entry);
+	}
+	mutex_unlock(&kvm->mem_access_table.mutex);
+}
+
+void kvm_arch_introspection_cleanup(struct kvm_vcpu *vcpu)
+{
+	int i;
+	for (i=0; i<=8; ++i) {
+		kvm_x86_ops.set_cr_monitor(vcpu, i, 0);
+	}
+	kvm_arch_clear_mem_access(vcpu->kvm);
+
+	kvm_set_syscall_hook(vcpu, 0);
+	kvm_x86_ops.set_monitor_trap_flag(vcpu, 0);
+	kvm_x86_ops.set_invlpg_monitor(vcpu, 0);
+	vcpu->kvm->mem_event_enabled = 0;
+	vcpu->vmcall_hook_enabled = 0;
+}
+
 static int __kvm_emulate_halt(struct kvm_vcpu *vcpu, int state, int reason)
 {
 	/*
@@ -9828,13 +10142,25 @@ int kvm_emulate_hypercall(struct kvm_vcp
 	unsigned long nr, a0, a1, a2, a3, ret;
 	int op_64_bit;
 
+	nr = kvm_rax_read(vcpu);
+	if(nr == 0xFACE)
+	{
+		const uint64_t original_rip = kvm_rip_read(vcpu);
+		if (vcpu->vmcall_hook_enabled)
+			kvm_deliver_vmcall_event(vcpu);
+
+		if (original_rip == kvm_rip_read(vcpu))
+			kvm_skip_emulated_instruction(vcpu);
+
+		return 0;
+	}
+
 	if (kvm_xen_hypercall_enabled(vcpu->kvm))
 		return kvm_xen_hypercall(vcpu);
 
 	if (kvm_hv_hypercall_enabled(vcpu))
 		return kvm_hv_hypercall(vcpu);
 
-	nr = kvm_rax_read(vcpu);
 	a0 = kvm_rbx_read(vcpu);
 	a1 = kvm_rcx_read(vcpu);
 	a2 = kvm_rdx_read(vcpu);
@@ -10987,6 +11313,26 @@ static int vcpu_run(struct kvm_vcpu *vcp
 			r = vcpu_block(vcpu);
 		}
 
+		/*
+		 * We shouldn't get here during an event
+		 * The vcpu will be blocked in event delivery
+		 */
+		if (atomic_read(&vcpu->pause_count)) {
+			// Release the vcpu
+			vcpu_put(vcpu);
+			mutex_unlock(&vcpu->mutex);
+
+			/* Wait until we're resumed */
+			wait_event(vcpu->pause_wait_queue,
+				atomic_read(&vcpu->pause_count) == 0);
+
+			/* Retake the vcpu */
+			if (mutex_lock_killable(&vcpu->mutex))
+				return -EINTR;
+
+			vcpu_load(vcpu);
+		}
+
 		if (r <= 0)
 			break;
 
@@ -11580,6 +11926,12 @@ static int __set_sregs(struct kvm_vcpu *
 	if (ret)
 		return ret;
 
+	if (kvm_rip_read(vcpu) == 0xfff0) {
+		vcpu->reboot_pending = true;
+	} else {
+		vcpu->reboot_pending = false;
+	}
+
 	if (mmu_reset_needed)
 		kvm_mmu_reset_context(vcpu);
 
@@ -13315,6 +13667,206 @@ int kvm_spec_ctrl_test_value(u64 value)
 }
 EXPORT_SYMBOL_GPL(kvm_spec_ctrl_test_value);
 
+void kvm_deliver_introspection_event(struct kvm_vcpu* vcpu,
+		struct kvm_introspection_event* event)
+{
+	if (!vcpu->introspection_mm)
+		return;
+
+	/*
+	 * Fill out the structure with registers and information
+	 */
+	event->event_id = atomic64_inc_return(&vcpu->kvm->event_id);
+	event->vcpu_id = vcpu->vcpu_id;
+    if (event->event_type != KVM_EVENT_SHUTDOWN) {
+	    __get_regs(vcpu, &event->regs);
+    	__get_sregs(vcpu, &event->sregs);
+    	kvm_vcpu_ioctl_x86_get_debugregs(vcpu, &event->debugregs);
+    }
+
+	mutex_lock(&vcpu->introspection_event_mutex);
+	if (unlikely(vcpu->introspection_event != NULL))
+		printk(KERN_ERR "kvm: Overwriting existing event %llu", event->event_id);
+
+	/*
+	 * Set the introspection event
+	 */
+	vcpu->introspection_event = event;
+	vcpu->introspection_event_pending_completion = true;
+	mutex_unlock(&vcpu->introspection_event_mutex);
+
+	/*
+	 * Release the vcpu while the event is being delivered.
+	 * If we don't do this, then ioctls will deadlock.
+	 */
+	if (likely(event->event_type != KVM_EVENT_SHUTDOWN)) {
+		atomic_inc(&vcpu->pause_count);
+		vcpu_put(vcpu);
+		mutex_unlock(&vcpu->mutex);
+	}
+
+	/*
+	 * Wake up the userland call to poll()
+	 */
+	wake_up(&vcpu->introspection_wait_queue);
+
+	/*
+	 * Block until KVM_COMPLETE_INTROSPECTION_EVENT
+	 */
+	wait_event(vcpu->introspection_event_completed_wait_queue,
+		vcpu->introspection_event_pending_completion == false);
+
+	/*
+	* If we're still being introspected, decrement the pause count.
+	* Then retake the vcpu and continue on about our business.
+	*/
+	if (likely(event->event_type != KVM_EVENT_SHUTDOWN)) {
+		if (vcpu->introspection_mm) {
+			kvm_vcpu_unpause(vcpu);
+		}
+
+		if (mutex_lock_killable(&vcpu->mutex))
+			return;
+		vcpu_load(vcpu);
+	}
+}
+
+void kvm_deliver_invlpg_event(struct kvm_vcpu *vcpu, uint64_t gva) {
+	if (vcpu->invlpg_hook) {
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_INVLPG;
+		event.invlpg.gva = gva;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+
+void kvm_deliver_mem_event(struct kvm_vcpu *vcpu, uint64_t gpa, u32 error_code) {
+	if (kvm_is_mem_event_enabled(vcpu->kvm)) {
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_MEM_ACCESS;
+		event.mem_event.gpa = gpa;
+		event.mem_event.error_code = error_code;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+
+void kvm_deliver_cr_read_event(struct kvm_vcpu *vcpu, int cr, u64 val)
+{
+	if (kvm_is_cr_hooked(vcpu, cr, KVM_MONITOR_CR_READ))
+	{
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_CR_READ;
+		event.cr_access.cr = cr;
+		event.cr_access.value = val;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_cr_read_event);
+
+void kvm_deliver_cr_write_event(struct kvm_vcpu *vcpu, int cr, u64 val)
+{
+	if (kvm_is_cr_hooked(vcpu, cr, KVM_MONITOR_CR_WRITE))
+	{
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_CR_WRITE;
+		event.cr_access.cr = cr;
+		event.cr_access.value = val;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_cr_write_event);
+
+void kvm_deliver_syscall_event(struct kvm_vcpu *vcpu, u64 return_address)
+{
+	if (vcpu->syscall_hook_enabled)
+	{
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_FAST_SYSCALL;
+		event.system_call.type = KVM_EVENT_SYSTEM_CALL_TYPE_SYSCALL;
+		event.system_call.return_address = return_address;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_syscall_event);
+
+void kvm_deliver_sysenter_event(struct kvm_vcpu *vcpu, u64 return_address)
+{
+	if (vcpu->syscall_hook_enabled)
+	{
+		struct kvm_introspection_event event;
+		event.event_type = KVM_EVENT_FAST_SYSCALL;
+		event.system_call.type = KVM_EVENT_SYSTEM_CALL_TYPE_SYSENTER;
+		event.system_call.return_address = return_address;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_sysenter_event);
+
+void kvm_deliver_sysret_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	if (vcpu->syscall_hook_enabled)
+	{
+		event.event_type = KVM_EVENT_FAST_SYSCALL_RET;
+		event.system_call_ret.type = KVM_EVENT_FAST_SYSCALL_RET;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_sysret_event);
+
+void kvm_deliver_sysexit_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	if (vcpu->syscall_hook_enabled)
+	{
+		event.event_type = KVM_EVENT_FAST_SYSCALL_RET;
+		event.system_call_ret.type = KVM_EVENT_SYSTEM_CALL_RET_TYPE_SYSEXIT;
+		kvm_deliver_introspection_event(vcpu, &event);
+	}
+}
+EXPORT_SYMBOL(kvm_deliver_sysexit_event);
+
+void kvm_deliver_vmcall_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	event.event_type = KVM_EVENT_HYPERCALL;
+	kvm_deliver_introspection_event(vcpu, &event);
+}
+EXPORT_SYMBOL(kvm_deliver_vmcall_event);
+
+void kvm_deliver_trap_event(struct kvm_vcpu *vcpu, int vector)
+{
+	struct kvm_introspection_event event;
+	event.event_type = KVM_EVENT_EXCEPTION;
+	event.trap.vector = vector;
+	kvm_deliver_introspection_event(vcpu, &event);
+}
+EXPORT_SYMBOL(kvm_deliver_trap_event);
+
+void kvm_deliver_monitor_trap_flag_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	event.event_type = KVM_EVENT_SINGLE_STEP;
+	kvm_deliver_introspection_event(vcpu, &event);
+}
+EXPORT_SYMBOL(kvm_deliver_monitor_trap_flag_event);
+
+void kvm_deliver_reboot_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	event.event_type = KVM_EVENT_REBOOT;
+	kvm_deliver_introspection_event(vcpu, &event);
+}
+EXPORT_SYMBOL(kvm_deliver_reboot_event);
+
+void kvm_deliver_shutdown_event(struct kvm_vcpu *vcpu)
+{
+	struct kvm_introspection_event event;
+	event.event_type = KVM_EVENT_SHUTDOWN;
+	kvm_deliver_introspection_event(vcpu, &event);
+}
+EXPORT_SYMBOL(kvm_deliver_shutdown_event);
+
 void kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code)
 {
 	struct kvm_mmu *mmu = vcpu->arch.walk_mmu;
Index: kvm-introvirt/kernel/arch/x86/kvm/x86.h
===================================================================
--- kvm-introvirt.orig/kernel/arch/x86/kvm/x86.h
+++ kvm-introvirt/kernel/arch/x86/kvm/x86.h
@@ -319,6 +319,30 @@ int x86_decode_emulated_instruction(stru
 				    void *insn, int insn_len);
 int x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
 			    int emulation_type, void *insn, int insn_len);
+
+static inline int kvm_is_cr_hooked(struct kvm_vcpu *vcpu, int cr, int mode)
+{
+	return (((vcpu->cr_monitor_mask) >> (cr * 2)) & mode) != 0;
+}
+
+static inline int kvm_is_mem_event_enabled(struct kvm *kvm)
+{
+	return kvm->mem_event_enabled;
+}
+
+void kvm_deliver_cr_read_event(struct kvm_vcpu *vcpu, int cr, u64 val);
+void kvm_deliver_cr_write_event(struct kvm_vcpu *vcpu, int cr, u64 val);
+void kvm_deliver_syscall_event(struct kvm_vcpu *vcpu, u64 return_address);
+void kvm_deliver_sysenter_event(struct kvm_vcpu *vcpu, u64 return_address);
+void kvm_deliver_sysret_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_sysexit_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_vmcall_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_trap_event(struct kvm_vcpu *vcpu, int vector);
+void kvm_deliver_invlpg_event(struct kvm_vcpu *vcpu, u64 gva);
+void kvm_deliver_monitor_trap_flag_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_reboot_event(struct kvm_vcpu *vcpu);
+void kvm_deliver_mem_event(struct kvm_vcpu *vcpu, u64 gpa, u32 error_code);
+
 fastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);
 
 extern u64 host_xcr0;
Index: kvm-introvirt/kernel/include/linux/kvm_host.h
===================================================================
--- kvm-introvirt.orig/kernel/include/linux/kvm_host.h
+++ kvm-introvirt/kernel/include/linux/kvm_host.h
@@ -35,6 +35,7 @@
 #include <linux/interval_tree.h>
 #include <linux/rbtree.h>
 #include <linux/xarray.h>
+#include <linux/hashtable.h>
 #include <asm/signal.h>
 
 #include <linux/kvm.h>
@@ -377,6 +378,26 @@ struct kvm_vcpu {
 #endif
 	bool preempted;
 	bool ready;
+
+	/* Introspection */
+	wait_queue_head_t introspection_wait_queue;
+	wait_queue_head_t introspection_event_completed_wait_queue;
+	struct mutex introspection_event_mutex;
+	struct kvm_introspection_event* introspection_event;
+	bool introspection_event_pending_completion;
+	u32 cr_monitor_mask;
+	bool invlpg_hook;
+	bool syscall_hook_enabled;
+	u64 shadow_msr_ia32_sysenter_cs;
+	bool vmcall_hook_enabled;
+	struct mm_struct *introspection_mm;
+	bool reboot_pending;
+	bool monitor_trap_flag;
+	u64 tid_address;
+
+	wait_queue_head_t pause_wait_queue;
+	atomic_t pause_count;
+
 	struct kvm_vcpu_arch arch;
 	struct kvm_vcpu_stat stat;
 	char stats_id[KVM_STATS_NAME_SIZE];
@@ -699,6 +720,17 @@ struct kvm_memslots {
 	int node_idx;
 };
 
+struct mem_access_entry {
+	struct hlist_node node;
+	u64 gfn;
+	uint8_t original_perms;
+};
+
+struct mem_access_table {
+	DECLARE_HASHTABLE(table, 4);
+	struct mutex mutex;
+};
+
 struct kvm {
 #ifdef KVM_HAVE_MMU_RWLOCK
 	rwlock_t mmu_lock;
@@ -806,6 +838,11 @@ struct kvm {
 	struct notifier_block pm_notifier;
 #endif
 	char stats_id[KVM_STATS_NAME_SIZE];
+
+	struct mem_access_table mem_access_table;
+	atomic64_t event_id;
+	struct mm_struct *introspection_mm;
+	bool mem_event_enabled;
 };
 
 #define kvm_err(fmt, ...) \
@@ -932,6 +969,9 @@ void kvm_destroy_vcpus(struct kvm *kvm);
 void vcpu_load(struct kvm_vcpu *vcpu);
 void vcpu_put(struct kvm_vcpu *vcpu);
 
+void kvm_vcpu_pause(struct kvm_vcpu *vcpu);
+void kvm_vcpu_unpause(struct kvm_vcpu *vcpu);
+
 #ifdef __KVM_HAVE_IOAPIC
 void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm);
 void kvm_arch_post_irq_routing_update(struct kvm *kvm);
@@ -1355,8 +1395,13 @@ void kvm_arch_vcpu_blocking(struct kvm_v
 void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu);
+
+void kvm_arch_introspection_cleanup(struct kvm_vcpu *vcpu);
+void kvm_deliver_shutdown_event(struct kvm_vcpu *vcpu);
+
 int kvm_vcpu_yield_to(struct kvm_vcpu *target);
 void kvm_vcpu_on_spin(struct kvm_vcpu *vcpu, bool yield_to_kernel_mode);
+void kvm_vcpu_check_pause(struct kvm_vcpu *vcpu); /* IntroVirt addition */
 
 void kvm_flush_remote_tlbs(struct kvm *kvm);
 
Index: kvm-introvirt/kernel/include/uapi/linux/kvm.h
===================================================================
--- kvm-introvirt.orig/kernel/include/uapi/linux/kvm.h
+++ kvm-introvirt/kernel/include/uapi/linux/kvm.h
@@ -576,6 +576,20 @@ struct kvm_translation {
 	__u8  pad[5];
 };
 
+/* for KVM_TRANSLATE_DTB */
+struct kvm_translation_dtb {
+	/* in */
+	__u64 linear_address;
+	__u64 directory_table_base;
+
+	/* out */
+	__u64 physical_address;
+	__u8  valid;
+	__u8  writeable;
+	__u8  usermode;
+	__u8  pad[5];
+};
+
 /* for KVM_S390_MEM_OP */
 struct kvm_s390_mem_op {
 	/* in */
@@ -944,6 +958,133 @@ struct kvm_ppc_resize_hpt {
 #define KVM_GET_MSR_FEATURE_INDEX_LIST    _IOWR(KVMIO, 0x0a, struct kvm_msr_list)
 
 /*
+ * Introspection API (KVM_CAP_INTROSPECTION)
+ */
+
+//
+// structs
+//
+struct kvm_introspection_patch_ver {
+    char buffer[64];
+};
+
+struct kvm_inject_trap {
+    __u32 vector;
+    __u32 error_code;
+    __u64 cr2;
+    int has_error;
+};
+
+struct kvm_ept_permissions {
+    __u64 gfn;
+    __u8 perms : 3;
+};
+
+struct kvm_cr_monitor {
+	int cr;
+	int mode; // Bitmask of KVM_MONITOR_CR_[READ/WRITE]
+};
+
+struct kvm_introspection_event {
+    __u64 event_id; // Increments with each event
+    int event_type; // KVM_EVENT_TYPE_
+    int vcpu_id;    // The ID of the VCPU that triggered the event
+
+    // Registers
+    struct kvm_regs regs;
+    struct kvm_sregs sregs;
+    struct kvm_debugregs debugregs;
+
+    union {
+        struct {
+            int cr;			// 0-8
+			int mode;		// KVM_MONITOR_CR_[READ/WRITE]
+            __u64 value;	// Value being loaded/stored
+        } cr_access;
+        struct {
+            int type; 	// KVM_EVENT_SYSTEM_CALL_TYPE_[X]
+            __u64 return_address;
+        } system_call;
+        struct {
+            int type;	// KVM_EVENT_SYSTEM_CALL_RET_TYPE_[X]
+			__u64 thread_id; // The address of the kernel stack base for this thread (or 0 if not available)
+        } system_call_ret;
+        struct {
+            int vector; // The vector that caused the trap, e.g. BP_VECTOR
+        } trap;
+		struct {
+			__u64 gpa;
+			__u32 error_code;
+		} mem_event;
+		struct {
+			__u64 gva;
+		} invlpg;
+    };
+};
+
+//
+// constants
+//
+#define KVM_EVENT_FAST_SYSCALL 0     // A system call event
+#define KVM_EVENT_FAST_SYSCALL_RET 1 // A system call return event
+#define KVM_EVENT_SW_INT 2           // A software interrupt event
+#define KVM_EVENT_SW_IRET 3          // A software interrupt return event
+#define KVM_EVENT_CR_READ 4          // A control register was read
+#define KVM_EVENT_CR_WRITE 5         // A control register was written to
+#define KVM_EVENT_MSR_READ 6         // An MSR was read
+#define KVM_EVENT_MSR_WRITE 7        // An MSR was written to
+#define KVM_EVENT_EXCEPTION 8        // An x86 exception event
+#define KVM_EVENT_MEM_ACCESS 9       // Hardware assisted paging violation (memory breakpoints)
+#define KVM_EVENT_SINGLE_STEP 10     // Single step event
+#define KVM_EVENT_HYPERCALL 11       // An intercepted hypercall
+#define KVM_EVENT_REBOOT 12          // The guest VM has rebooted
+#define KVM_EVENT_SHUTDOWN 13        // The guest VM has shutdown
+#define KVM_EVENT_INVLPG 14          // INVLPG instruction was executed
+
+#define KVM_EVENT_SYSTEM_CALL_TYPE_SYSCALL	1
+#define KVM_EVENT_SYSTEM_CALL_TYPE_SYSENTER	2
+
+#define KVM_EVENT_SYSTEM_CALL_RET_TYPE_SYSRET	1
+#define KVM_EVENT_SYSTEM_CALL_RET_TYPE_SYSEXIT	2
+
+#define KVM_MONITOR_CR_READ      (1u << 0)
+#define KVM_MONITOR_CR_WRITE     (1u << 1)
+
+#define KVM_CAP_INTROSPECTION 20150308
+#define KVM_INTROSPECTION_API_VERSION 5
+
+#ifndef KVM_INTROSPECTION_PATCH_VERSION
+#define KVM_INTROSPECTION_PATCH_VERSION "UNKNOWN_INTROVIRT_VERSION"
+#endif
+
+//
+// ioctls
+//
+
+// kvm dev level
+#define KVM_ATTACH_VM _IOW(KVMIO, 0xd0, pid_t)
+#define KVM_GET_INTROSPECTION_PATCH_VERSION _IOR(KVMIO, 0xd1, struct kvm_introspection_patch_ver)
+
+// VM Level
+#define KVM_ATTACH_VCPU _IOW(KVMIO, 0xd2, unsigned long)
+#define KVM_SET_MEM_ACCESS_ENABLED _IOW(KVMIO, 0xd4, unsigned long)
+#define KVM_SET_MEM_ACCESS _IOW(KVMIO, 0xd5, struct kvm_ept_permissions)
+
+// VCPU level
+#define KVM_SET_CR_MONITOR _IOW(KVMIO, 0xd6, struct kvm_cr_monitor)
+#define KVM_SET_SYSCALL_HOOK _IOW(KVMIO, 0xd7, unsigned long)
+#define KVM_SET_VMCALL_HOOK _IOW(KVMIO, 0xd8, unsigned long)
+#define KVM_VCPU_PAUSE _IO(KVMIO, 0xd9)
+#define KVM_VCPU_UNPAUSE _IO(KVMIO, 0xda)
+#define KVM_GET_INTROSPECTION_EVENT _IOR(KVMIO, 0xdb, struct kvm_introspection_event)
+#define KVM_COMPLETE_INTROSPECTION_EVENT _IO(KVMIO, 0xdc)
+#define KVM_INJECT_TRAP _IOW(KVMIO, 0xdd, struct kvm_inject_trap)
+#define KVM_SET_MONITOR_TRAP_FLAG _IOW(KVMIO, 0xde, unsigned long)
+#define KVM_SET_INVLPG_HOOK _IOW(KVMIO, 0xdf, unsigned long)
+#define KVM_VCPU_INJECT_SYSCALL _IO(KVMIO, 0xf1)
+#define KVM_VCPU_INJECT_SYSENTER _IO(KVMIO, 0xf2)
+
+/*
  * Extension capability list.
  */
 #define KVM_CAP_IRQCHIP	  0
Index: kvm-introvirt/kernel/virt/kvm/kvm_main.c
===================================================================
--- kvm-introvirt.orig/kernel/virt/kvm/kvm_main.c
+++ kvm-introvirt/kernel/virt/kvm/kvm_main.c
@@ -478,6 +478,14 @@ static void kvm_vcpu_init(struct kvm_vcp
 	/* Fill the stats id string for the vcpu */
 	snprintf(vcpu->stats_id, sizeof(vcpu->stats_id), "kvm-%d/vcpu-%d",
 		 task_pid_nr(current), id);
+
+	// Introspection
+	mutex_init(&vcpu->introspection_event_mutex);
+	init_waitqueue_head(&vcpu->introspection_event_completed_wait_queue);
+	init_waitqueue_head(&vcpu->introspection_wait_queue);
+	init_waitqueue_head(&vcpu->pause_wait_queue);
+	vcpu->introspection_event = NULL;
+	vcpu->reboot_pending = false;
 }
 
 static void kvm_vcpu_destroy(struct kvm_vcpu *vcpu)
@@ -1228,6 +1236,9 @@ static struct kvm *kvm_create_vm(unsigne
 	preempt_notifier_inc();
 	kvm_init_pm_notifier(kvm);
 
+	mutex_init(&kvm->mem_access_table.mutex);
+	hash_init(kvm->mem_access_table.table);
+
 	return kvm;
 
 out_err:
@@ -1371,6 +1382,18 @@ static int kvm_vm_release(struct inode *
 	return 0;
 }
 
+static int kvm_vm_introvirt_release(struct inode *inode, struct file *filp)
+{
+    struct kvm *kvm = filp->private_data;
+
+    // IntroVirt tool detaching
+
+    kvm->introspection_mm = NULL;
+
+    kvm_put_kvm(kvm);
+    return 0;
+}
+
 /*
  * Allocation size is twice as large as the actual dirty bitmap size.
  * See kvm_vm_ioctl_get_dirty_log() why this is needed.
@@ -3457,6 +3480,9 @@ bool kvm_vcpu_block(struct kvm_vcpu *vcp
 		if (kvm_vcpu_check_block(vcpu) < 0)
 			break;
 
+		if (unlikely(atomic_read(&vcpu->pause_count) > 0))
+			break;
+
 		waited = true;
 		schedule();
 	}
@@ -3846,16 +3872,80 @@ static int kvm_vcpu_release(struct inode
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
 
+	if (vcpu->vcpu_id == 0)
+		kvm_deliver_shutdown_event(vcpu);
+
 	kvm_put_kvm(vcpu->kvm);
 	return 0;
 }
 
+static int kvm_vcpu_introvirt_release(struct inode *inode, struct file *filp)
+{
+	struct kvm_vcpu *vcpu = filp->private_data;
+
+	kvm_vcpu_pause(vcpu);
+
+	mutex_lock(&vcpu->mutex);
+	vcpu_load(vcpu);
+
+	// Turn off introspection features
+	kvm_arch_introspection_cleanup(vcpu);
+
+	vcpu->introspection_event = NULL;
+	vcpu->introspection_event_pending_completion = false;
+	vcpu->introspection_mm = NULL;
+
+	// Resume the VCPU
+	vcpu_put(vcpu);
+	mutex_unlock(&vcpu->mutex);
+
+	// Wake up anyone that's paused
+	atomic_set(&vcpu->pause_count, 0);
+	wake_up_all(&vcpu->pause_wait_queue);
+
+	// If an introspection event is being delivered, wake that up
+	wake_up(&vcpu->introspection_event_completed_wait_queue);
+
+	kvm_put_kvm(vcpu->kvm);
+
+	return 0;
+}
+
+// Poll for introspection events
+unsigned int kvm_vcpu_poll(struct file *filp, poll_table *wait)
+{
+	struct kvm_vcpu* vcpu = filp->private_data;
+	unsigned int mask = 0;
+
+	poll_wait(filp, &vcpu->introspection_wait_queue, wait);
+
+	if(mutex_lock_interruptible(&vcpu->introspection_event_mutex))
+		return -ERESTARTSYS;
+
+	if(vcpu->introspection_event != NULL)
+		mask |= POLLIN;
+
+	mutex_unlock(&vcpu->introspection_event_mutex);
+
+	return mask;
+}
+
 static const struct file_operations kvm_vcpu_fops = {
 	.release        = kvm_vcpu_release,
 	.unlocked_ioctl = kvm_vcpu_ioctl,
 	.mmap           = kvm_vcpu_mmap,
-	.llseek		= noop_llseek,
+	.llseek		    = noop_llseek,
 	KVM_COMPAT(kvm_vcpu_compat_ioctl),
+	.poll           = kvm_vcpu_poll,
+};
+
+static struct file_operations kvm_vcpu_introvirt_fops = {
+	.release        = kvm_vcpu_introvirt_release,
+	.unlocked_ioctl = kvm_vcpu_ioctl,
+	.mmap           = kvm_vcpu_mmap,
+	.llseek         = noop_llseek,
+	KVM_COMPAT(kvm_vcpu_compat_ioctl),
+	.poll           = kvm_vcpu_poll,
 };
 
 /*
@@ -3869,6 +3959,11 @@ static int create_vcpu_fd(struct kvm_vcp
 	return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
 }
 
+static int create_vcpu_introvirt_fd(struct kvm_vcpu *vcpu)
+{
+	return anon_inode_getfd("kvm-vcpu", &kvm_vcpu_introvirt_fops, vcpu, O_RDWR | O_CLOEXEC);
+}
+
 #ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS
 static int vcpu_get_pid(void *data, u64 *val)
 {
@@ -4014,6 +4109,35 @@ vcpu_decrement:
 	return r;
 }
 
+/* Attach for introspection */
+static int kvm_vm_ioctl_attach_vcpu(struct kvm *kvm, u32 id) {
+	struct kvm_vcpu* vcpu;
+	int r;
+
+	kvm_get_kvm(kvm);
+
+	r = -ESRCH;
+	vcpu = kvm_get_vcpu_by_id(kvm, id);
+	if(!vcpu)
+		goto out_err;
+
+	r = -EBUSY;
+	if (vcpu->introspection_mm)
+		goto out_err;
+
+	r = create_vcpu_introvirt_fd(vcpu);
+	if(r < 0)
+		goto out_err;
+
+	vcpu->introspection_mm = current->mm;
+	goto out;
+
+out_err:
+	kvm_put_kvm(kvm);
+out:
+	return r;
+}
+
 static int kvm_vcpu_ioctl_set_sigmask(struct kvm_vcpu *vcpu, sigset_t *sigset)
 {
 	if (sigset) {
@@ -4025,6 +4149,31 @@ static int kvm_vcpu_ioctl_set_sigmask(st
 	return 0;
 }
 
+void kvm_vcpu_pause(struct kvm_vcpu *vcpu)
+{
+	if (atomic_inc_return(&vcpu->pause_count) == 1) {
+		/*
+		 * Wake up the vcpu
+		 */
+		kvm_vcpu_kick(vcpu);
+
+		/*
+		 * Wait for the vcpu to pause
+		 */
+		if (mutex_lock_killable(&vcpu->mutex)) {
+			return;
+		}
+		mutex_unlock(&vcpu->mutex);
+	}
+}
+
+void kvm_vcpu_unpause(struct kvm_vcpu *vcpu)
+{
+	if(atomic_dec_and_test(&vcpu->pause_count)) {
+		wake_up_all(&vcpu->pause_wait_queue);
+	}
+}
+
 static ssize_t kvm_vcpu_stats_read(struct file *file, char __user *user_buffer,
 			      size_t size, loff_t *offset)
 {
@@ -4084,8 +4233,8 @@ static long kvm_vcpu_ioctl(struct file *
 	struct kvm_fpu *fpu = NULL;
 	struct kvm_sregs *kvm_sregs = NULL;
 
-	if (vcpu->kvm->mm != current->mm || vcpu->kvm->vm_dead)
-		return -EIO;
+	/*if (vcpu->kvm->mm != current->mm || vcpu->kvm->vm_dead)
+		return -EIO;*/
 
 	if (unlikely(_IOC_TYPE(ioctl) != KVMIO))
 		return -EINVAL;
@@ -4098,6 +4247,69 @@ static long kvm_vcpu_ioctl(struct file *
 	if (r != -ENOIOCTLCMD)
 		return r;
 
+	// IOCTLs that take place without locking vcpu->mutex
+	switch (ioctl) {
+	case KVM_VCPU_PAUSE: {
+		r = 0;
+		kvm_vcpu_pause(vcpu);
+		return r;
+	}
+	case KVM_VCPU_UNPAUSE: {
+		r = 0;
+		kvm_vcpu_unpause(vcpu);
+		return r;
+	}
+	case KVM_GET_INTROSPECTION_EVENT: {
+		if(mutex_lock_interruptible(&vcpu->introspection_event_mutex))
+			return -ERESTARTSYS;
+
+		r = -ENOENT;
+		if(vcpu->introspection_event) {
+			r = -EFAULT;
+			if(!copy_to_user(argp, vcpu->introspection_event,
+					sizeof(struct kvm_introspection_event))) {
+				r = 0;
+				vcpu->introspection_event = NULL;
+			}
+		}
+
+		mutex_unlock(&vcpu->introspection_event_mutex);
+		return r;
+	}
+	case KVM_COMPLETE_INTROSPECTION_EVENT: {
+		if(mutex_lock_interruptible(&vcpu->introspection_event_mutex))
+			return -ERESTARTSYS;
+
+		r = -EINVAL;
+		if(vcpu->introspection_event_pending_completion) {
+			vcpu->introspection_event_pending_completion = false;
+			r = 0;
+
+			// Wake up the VCPU
+			wake_up(&vcpu->introspection_event_completed_wait_queue);
+		}
+
+		mutex_unlock(&vcpu->introspection_event_mutex);
+
+		return r;
+	}
+	}
+
+	/*
+	 * Block Qemu while we're paused.
+	 *
+	 * This almost never happens. Qemu would have
+	 * to be in userland while a kvm_vcpu_pause()
+	 * is issued. Normally we kick it and pause after
+	 * it wakes.
+	 */
+	if (unlikely(vcpu->kvm->mm == current->mm
+		&& atomic_read(&vcpu->pause_count))) {
+			/* Wait until we're resumed */
+			wait_event(vcpu->pause_wait_queue,
+				atomic_read(&vcpu->pause_count) == 0);
+	}
+
 	if (mutex_lock_killable(&vcpu->mutex))
 		return -EINTR;
 	switch (ioctl) {
@@ -4758,8 +4970,8 @@ static long kvm_vm_ioctl(struct file *fi
 	void __user *argp = (void __user *)arg;
 	int r;
 
-	if (kvm->mm != current->mm || kvm->vm_dead)
-		return -EIO;
+	/*if (kvm->mm != current->mm || kvm->vm_dead)
+		return -EIO;*/
 	switch (ioctl) {
 	case KVM_CREATE_VCPU:
 		r = kvm_vm_ioctl_create_vcpu(kvm, arg);
@@ -4936,6 +5148,9 @@ static long kvm_vm_ioctl(struct file *fi
 	case KVM_GET_STATS_FD:
 		r = kvm_vm_ioctl_get_stats_fd(kvm);
 		break;
+	case KVM_ATTACH_VCPU:
+		r = kvm_vm_ioctl_attach_vcpu(kvm, arg);
+		break;
 	default:
 		r = kvm_arch_vm_ioctl(filp, ioctl, arg);
 	}
@@ -5023,11 +5238,99 @@ static long kvm_vm_compat_ioctl(struct f
 }
 #endif
 
+static void kvm_vm_munmap(struct vm_area_struct *vma) {
+	struct page* page = vma->vm_private_data;
+	put_page(page);
+}
+
+static struct vm_operations_struct kvm_vm_mapping_ops = {
+    .close        = kvm_vm_munmap,
+};
+
+static int kvm_vm_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct kvm *kvm = vma->vm_file->private_data;
+	const gfn_t first_page = vma->vm_start >> PAGE_SHIFT;
+	const gfn_t last_page = (vma->vm_end - 1) >> PAGE_SHIFT;
+	const unsigned int page_count = (last_page - first_page) + 1;
+	unsigned long addresses[16];
+	struct page* pages[16];
+	int i;
+
+	vma->vm_ops = &kvm_vm_mapping_ops;
+
+	if ( page_count > 1 ) {
+		return -E2BIG;
+	}
+
+	/*
+	 * Translate each guest gfn to an address
+	 */
+	for(i = 0; i < page_count; ++i) {
+		gfn_t gfn = vma->vm_pgoff + i;
+		uint64_t addr = gfn_to_hva(kvm, gfn);
+		if(kvm_is_error_hva(addr)) {
+			if(addr == KVM_HVA_ERR_BAD) {
+				return -EFAULT;
+			} else if(addr == KVM_HVA_ERR_RO_BAD) {
+				return -EPERM;
+			} else {
+				return -EINVAL;
+			}
+		}
+		addresses[i] = addr;
+	}
+
+	/*
+	 * Translate each address to page structs
+	 */
+	down_read(&kvm->mm->mmap_lock);
+	for(i = 0; i < page_count; ++i) {
+		int npages;
+		npages = get_user_pages_remote(kvm->mm, addresses[i], 1, FOLL_WRITE,
+				&pages[i], NULL);
+
+		if(unlikely(npages != 1)) {
+			up_read(&kvm->mm->mmap_lock);
+			return VM_FAULT_SIGBUS;
+		}
+	}
+	up_read(&kvm->mm->mmap_lock);
+
+	/*
+	 * Do the actual mapping
+	 */
+	down_write(&kvm->mm->mmap_lock);
+	for(i = 0; i < page_count; ++i) {
+		int r;
+
+		r = remap_pfn_range(vma, vma->vm_start + (0x1000 * i),
+				page_to_pfn(pages[i]), 0x1000,
+				vma->vm_page_prot);
+		if(r)
+			printk(KERN_ALERT "vm_insert_page() failed : %d\n", r);
+
+		vma->vm_private_data = pages[i];
+	}
+	up_write(&kvm->mm->mmap_lock);
+
+	return 0;
+}
+
 static const struct file_operations kvm_vm_fops = {
 	.release        = kvm_vm_release,
 	.unlocked_ioctl = kvm_vm_ioctl,
-	.llseek		= noop_llseek,
+	.llseek		    = noop_llseek,
+	KVM_COMPAT(kvm_vm_compat_ioctl),
+	.mmap           = kvm_vm_mmap,
+};
+
+static struct file_operations kvm_vm_introvirt_fops = {
+    .release        = kvm_vm_introvirt_release,
+    .unlocked_ioctl = kvm_vm_ioctl,
 	KVM_COMPAT(kvm_vm_compat_ioctl),
+    .llseek         = noop_llseek,
+    .mmap           = kvm_vm_mmap,
 };
 
 bool file_is_kvm(struct file *file)
@@ -5079,10 +5382,39 @@ put_fd:
 	return r;
 }
 
+/*
+ * Find a VM based on the PID
+ */
+struct kvm* get_vm_by_pid(pid_t pid)
+{
+	struct kvm *rv;
+	struct kvm *kvm;
+
+	rv = NULL;
+
+	mutex_lock(&kvm_lock);
+	list_for_each_entry(kvm, &vm_list, vm_list)
+	{
+		if(kvm->mm && kvm->mm->owner)
+		{
+			if(kvm->mm->owner->pid == pid)
+			{
+				rv = kvm;
+				break;
+			}
+		}
+	}
+
+	mutex_unlock(&kvm_lock);
+
+	return rv;
+}
+
 static long kvm_dev_ioctl(struct file *filp,
 			  unsigned int ioctl, unsigned long arg)
 {
 	int r = -EINVAL;
+	void __user *argp = (void __user *)arg;
 
 	switch (ioctl) {
 	case KVM_GET_API_VERSION:
@@ -5112,6 +5444,48 @@ static long kvm_dev_ioctl(struct file *f
 	case KVM_TRACE_DISABLE:
 		r = -EOPNOTSUPP;
 		break;
+	case KVM_ATTACH_VM: {
+		struct kvm* kvm;
+
+		r = -ESRCH;
+		kvm = get_vm_by_pid(arg);
+		if(!kvm)
+			goto out;
+
+		r = -EBUSY;
+		if(kvm->introspection_mm)
+		    goto out;
+
+		/* Increment the counter */
+		kvm_get_kvm(kvm);
+
+		/* Get handle to the VM */
+		r = anon_inode_getfd("kvm-vm", &kvm_vm_introvirt_fops, kvm,
+				O_RDWR | O_CLOEXEC);
+
+		kvm->introspection_mm = current->mm;
+
+		if(r < 0) {
+			kvm_put_kvm(kvm);
+			kvm->introspection_mm = NULL;
+		}
+		break;
+	}
+	case KVM_GET_INTROSPECTION_PATCH_VERSION: {
+		const char* str = KVM_INTROSPECTION_PATCH_VERSION;
+		const int len = strlen(str) + 1;
+
+		if (len > sizeof(struct kvm_introspection_patch_ver)) {
+			r = -ETOOSMALL;
+			goto out;
+		}
+
+		r = -EFAULT;
+		if(!copy_to_user(argp, str, len)) {
+			r = 0;
+		}
+		break;
+	}
 	default:
 		return kvm_arch_dev_ioctl(filp, ioctl, arg);
 	}
